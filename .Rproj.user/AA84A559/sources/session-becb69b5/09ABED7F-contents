---
title: "Ex2"
format: html
---

# Data summaries and visualizations

```{r, echo=FALSE}
library(ISLR2)
library(tidyverse)
library(ggplot2)
library(MASS)
library(ROCR)
```

```{r}
df <- Default

summary(df)
```

```{r}
str(df)
```

Everything is already coded as needed.

We can check some other features of the data by plotting scatterplots of association between the differennt variables.

```{r}
pairs(df)
```

From this we learn that the income and balance seem to be slightly correlated. We can check this formally by using the `cor` function.

```{r}
cor(df$balance, df$income)
```

Also, we can check the distribution of some variables based on the dependent variable of interest (i.e., default).

```{r}
boxplot(balance~default, data = df)
```

There are some outliers in both groups, but the group that defaulted seems to have a higher outstanding balance compared to the group of people that did not. Therefore, balance could have a high discriminative power as it appears to be quite good at separating those who defaulted from those who did not.

```{r, fig.cap='example'}
boxplot(income~default, data = df)
```

For what concerns the variable income, we can see that the median is pretty much the same across groups so it might not necessarily be a good predictor of default.

# Train/Test Split

Now we are going to randomly split the data into a train and test portion.

```{r}
set.seed(11)
n <- nrow(df)
indices <- seq(1,n)
train <- sample(indices, round(n*.75), replace = F) #<1>

train_df <- df[train,]
test_df <- df[-train,]
```

1.  `sample` handles the *random* request made by the exercise. Still, the `set.seed` makes sure that the same splits are obtained across iterations.

# Logistic regression

```{r}
lr <- glm(default~., data = train_df, family = 'binomial')

lr_pred <- predict(lr, newdata = test_df, type = 'response')

lr_classes <- ifelse(lr_pred > .5, 'Yes', 'No')

mean(lr_classes != test_df$default)
```

# LDA

```{r}
lda_mod <- lda(default~., data = train_df)

lda_pred <- predict(lda_mod, newdata = test_df, type = 'response')

#lda_classes <- ifelse(lda_pred > .5, 'Yes', 'No')

mean(lda_pred$class != test_df$default)
```

# QDA

```{r}
qda_mod <- qda(default~., data = train_df)

qda_pred <- predict(qda_mod, newdata = test_df)

#lda_classes <- ifelse(lda_pred > .5, 'Yes', 'No')

mean(qda_pred$class != test_df$default)
```

# Very good models?

We will now check if the models fitted above are very good or if they have some issues that cannot be spotted just by looking at error metrics.

## Compute performance of null model

```{r}
pred_null <- rep('No',nrow(test_df))

mean(pred_null != test_df$default)
```

A model that predicts always to the non-default class already gets a very small error rate! This might be a symptom of imbalanced classes in the data.

We can check how default is distributed in the dataset.

```{r}
table(df$default)
```

From the result above we can definitely say that there is a significant imbalance between the two classes and this impacts the performance of the three models.

## Metrics for Logistic Regression

```{r}
(lr_conf <- table(lr_classes, test_df$default))
```

```{r}
tp <- lr_conf[1,1]
fn <- lr_conf[2,1]
(lr_sensitivity <-tp/(tp+fn))
```

```{r}
tn <- lr_conf[2,2]
fp <- lr_conf[1,2]
(lr_specificity <- tn/(tn+fp))
```

## Metrics for LDA

```{r}
(lda_conf <- table(lda_pred$class, test_df$default))
```

```{r}
tp <- lda_conf[1,1]
fn <- lda_conf[2,1]
(lda_sensitivity <-tp/(tp+fn))
```

```{r}
tn <- lda_conf[2,2]
fp <- lda_conf[1,2]
(lda_specificity <- tn/(tn+fp))
```

## Metrics for QDA

```{r}
(qda_conf <- table(qda_pred$class, test_df$default))
```

```{r}
tp <- qda_conf[1,1]
fn <- qda_conf[2,1]
(qda_sensitivity <-tp/(tp+fn))
```

```{r}
tn <- qda_conf[2,2]
fp <- qda_conf[1,2]
(qda_specificity <- tn/(tn+fp))
```

We conclude that all models have a high sensitivity but a very low specificity.

# ROC Curves

```{r}
predob <- ROCR::prediction(lr_pred, test_df$default)

perf <- ROCR::performance(predob, 'tpr', 'fpr')

plot(perf, main = 'Logistic Regression')
abline(0,1,lty=2)
```

```{r}
predob <- ROCR::prediction(lda_pred$x, test_df$default)

perf <- ROCR::performance(predob, 'tpr', 'fpr')

plot(perf, main = 'LDA')
abline(0,1,lty=2)
```

```{r}
predob <- ROCR::prediction(qda_pred$posterior, test_df$default)

perf <- ROCR::performance(predob, 'tpr', 'fpr')

plot(perf, main = 'QDA')
abline(0,1,lty=2)
```

```{r}
all_sens <- all_fpr <- c()

for (prob_cutoff in seq(0, 1, length.out=500)) {
    sens <- sum(lr_pred >= prob_cutoff & test_df$default == "Yes") / sum(test_df$default == "Yes")
    fpr <- sum(lr_pred >= prob_cutoff & test_df$default == "No") / sum(test_df$default == "No")

    all_sens <- append(all_sens, sens)
    all_fpr <- append(all_fpr, fpr)
}

```

```{r}
plot(all_fpr, all_sens, type = "l", xlab = "False positive rate", ylab = "True positive rate", main = 'Logistic Regression')
abline(0, 1, col = "gray", lty = 2)
```

```{r}
# compute the AUC
height <- (all_sens[-1] + all_sens[-length(all_sens)]) / 2
width <- -diff(all_fpr) 
# alternatively:
# width <- diff(rev(all_fpr))
AUC_lr <- sum(height * width)
```

```{r}
all_sens <- all_fpr <- c()

for (prob_cutoff in seq(0, 1, length.out=500)) {
    sens <- sum(lda_pred$x >= prob_cutoff & test_df$default == "Yes") / sum(test_df$default == "Yes")
    fpr <- sum(lda_pred$x >= prob_cutoff & test_df$default == "No") / sum(test_df$default == "No")

    all_sens <- append(all_sens, sens)
    all_fpr <- append(all_fpr, fpr)
}

```

```{r}
plot(all_fpr, all_sens, type = "l", xlab = "False positive rate", ylab = "True positive rate", main = 'LDA')
abline(0, 1, col = "gray", lty = 2)
```

```{r}
# compute the AUC
height <- (all_sens[-1] + all_sens[-length(all_sens)]) / 2
width <- -diff(all_fpr) 
# alternatively:
# width <- diff(rev(all_fpr))
AUC_lda <- sum(height * width)
```

## Not sure about this

```{r}
all_sens <- all_fpr <- c()

for (prob_cutoff in seq(0, 1, length.out=500)) {
    sens <- sum(qda_pred$posterior >= prob_cutoff & test_df$default == "Yes") / sum(test_df$default == "Yes")
    fpr <- sum(qda_pred$posterior >= prob_cutoff & test_df$default == "No") / sum(test_df$default == "No")

    all_sens <- append(all_sens, sens)
    all_fpr <- append(all_fpr, fpr)
}

```

```{r}
plot(all_fpr, all_sens, type = "l", xlab = "False positive rate", ylab = "True positive rate", main = 'QDA')
abline(0, 1, col = "gray", lty = 2)
```
