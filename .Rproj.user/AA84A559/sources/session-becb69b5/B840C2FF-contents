---
title: 'Statistical Learning, Lab #1: Classification'
author: "Marco Chierici"
date: "March 10, 2025"
format:
    html:
        theme: cosmo
        df-print: paged
        toc: true
        toc-location: left
knitr:
    opts_chunk:
        warning: false
        message: false
        tidy: true
        tidy.opts:
            width.cutoff: 60
editor_options:
  chunk_output_type: console
---

# The Stock Market Data

We start putting in practice the concepts learned during the lessons by looking at the `Smarket` data, part of the `ISLR2` library. The data set contains percentage returns for the S&P 500 stock index over 1,250 days (*observations*), from 2001 to 2005. Each observation is characterized by a number of *predictors*: `Lag1` to `Lag5` (percentage returns for the five previous days), `Volume` (the number of shares traded on the previous day), `Today` (percentage return on that date), `Direction` (whether the market was Up or Down on that date).

It is always good to literally look at the data. We can use RStudio's `View()` function to visualize the Stock Market dataframe in the Source panel:

```{r}
library(tidyverse)
library(ISLR2)
# View(Smarket)
```

A good alternative with small dataframes is the `head()` function:

```{r}
head(Smarket)
```

Or we can view it as a *tibble*, which is an improved and more efficient version of a dataframe:

```{r}
as_tibble(Smarket)
```

What about variable names, dimensions?

```{r}
names(Smarket)
dim(Smarket)
summary(Smarket)
```

**Hint:** for convenience, we can copy the `Smarket` dataframe to a new variable with a shorter name, and then forget about the `Smarket` object. Most importantly, we can modify the dataframe stored in the new variable knowing that if we mess up we can copy `Smarket` over and start again.

```{r}
dataf <- Smarket
```

## Data exploration

One of the first things to do is to explore the relationships between the variables. We can do that quickly using the pairs function, which produces a scatterplot matrix:

```{r}
pairs(dataf)
```

This gives us a rough, qualitative idea. Let's quantify by computing pairwise correlations:

```{r}
#| eval: false
cor(dataf)
```

Of course if we try that we get an error: there is a qualitative, non-numeric column (Direction) in the dataframe. Remove it and recompute the correlations:

```{r}
cor(dataf[, -9])
```

Relying on raw indexing to (de)select columns can be tricky: here's a better way to do it using `dplyr` (part of the `tidyverse`).

```{r}
cor(dplyr::select(dataf, -Direction))
```

By looking at the matrix, we see that the correlations are mostly close to zero, except for those between `Year` and `Volume`.

Sometimes it is useful to directly access the columns of a dataframe without using the usual syntax `dataf$column_name`. We can do that by "attaching" the dataframe to the R search path:

```{r}
attach(dataf)
print(head(Volume, n = 10))
```

Now let's have a look at the Volume variable, by plotting its values:

```{r}
plot(Volume)
```

What if we wanted to plot the Volume values by year?

```{r}
plot(Year, Volume)
```

Probably not the smartest idea. Let's try to make this plot more informative: for example, we can plot the *mean* Volume by year. A quick way to do this is using the `aggregate` function:

```{r}
Volume_means <- aggregate(Volume, list(Year), mean)
Volume_means
```

(**Hint:** get used to look at the contents of a newly created object, like I did above by typing `Volume_means`. This allows you to detect potential errors in advance!)

Now, let's plot the aggregated volumes:

```{r}
plot(Volume_means)
```

What if we still wanted to plot all `Volume` values by year? Here is a solution involving `ggplot2` and *jittering* (i.e., adding random noise to data to prevent overplotting).

```{r}
ggplot(dataf, aes(Year, Volume)) +
    geom_jitter(height = 0)
```

## Logistic Regression

### Fitting a model

We now want to predict the `Direction` target variable using a multivariate Logistic Regression model with `Lag1`-`Lag5` and `Volume` as predictors.

Let's set up the logistic regression model:

```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
    data = dataf,
    family = binomial
) # family=binomial selects a Logistic Regression model
glm.fits
```

We can inspect the `glm` object to retrieve more information:

```{r}
summary(glm.fits)
```

Note the use of R's "formula" syntax, `target ~ predictor1 + predictor2 + ... + predictorN`.

An alternative way of fitting a logistic regression (LR) model is through the library `tidymodels`: let's see how.

Using `tidymodels`' logic, we need to create a LR model *specification*, which we then pass to a `fit` function from the library `parsnip`.

The specification requires you to define three ingredients:

1.  The functional form of model (e.g., logistic regression);
2.  The R package that will be used as the computational engine;
3.  The mode used in the model (classification/regression).

```{r}
#| tidy: false

library(tidymodels)
lr_spec <- logistic_reg() %>% # define a generalized linear model for binary outcomes
    set_engine("glm") %>% # declare which package will be used to fit the model
    set_mode("classification") # set model's mode to classification
```

The specification does not perform any computation.

The actual model fit is performed by a call to `parsnip`'s `fit()` on our `lr_spec` model specification:

```{r}
lr_fit <- lr_spec %>%
    fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
        data = dataf
    )

lr_fit
```

Note that:

-   The fit is done using `glm()`;
-   The call to `fit()` uses the same formula syntax we used before.

The `lr_fit` object contains different slots: the `fit` slot stores the results of the `glm()` fit, and we can inspect it using `summary()`.

```{r}
#| tidy: false

lr_fit %>%
    pluck("fit") %>% # this function from the purrr library selects the "fit" slot
    summary()
```

There is a convenient way to programmatically extract parameter estimates, standard errors, and model statistics using the function `tidy()`:

```{r}
lr_stats <- tidy(lr_fit)
lr_stats
```

For example, we can extract the coefficients:

```{r}
lr_stats$estimate
```

We can use `glance()` to extract the model statistics:

```{r}
glance(lr_fit)
```

Using the standard `glm()` output, which we saved to the `glm.fits` object, we can achieve the same in the following alternative ways:

```{r}
# accesses only coefficients
coef(glm.fits)

# same as coef(glm.fits)
summary(glm.fits)$coef[, 1]

# same as tidy(lr_fit)
summary(glm.fits)$coef
```

In the following, we'll focus on the base R approach for model fitting.

### Getting predictions

Predictions are done using the `predict()` function: `predict(glm.fits, type="response")`.

If we don't specify any `type`, the default is to return predictions on the scale of the linear predictors (i.e., log-odds); with `type="response"`, the predictions are on the scale of the response variable (so they are actual probabilities).

```{r}
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
```

Probabilities of what, exactly? Let's see how our response is coded:

```{r}
contrasts(Direction)
```

For R, `Up` is coded as 1: so, `glm.probs` are the **probabilities of Direction being Up**. We want actual labels instead of probabilities (and we are using our standard 0.5 threshold):

```{r}
glm.pred <- rep("Down", nrow(dataf)) # create a "placeholder" filled with as many Down values as the number of observations (rows) in dataf
glm.pred[glm.probs > 0.5] <- "Up" # replace with Up values according to the glm.probs threshold
```

We could achieve all of the above using the `tidymodels` approach:

```{r}
predict(lr_fit, new_data = dataf, type = "prob")
```

Note that:

-   we need to specify the `new_data` argument;
-   `type="prob"` instead of `type="response"`;
-   we get probabilities for both classes: this is quite redundant for binary classification, but would become useful for multiclassification tasks.

The actual predicted labels can be obtained using `type="class"` instead:

```{r}
predict(lr_fit, new_data = dataf, type = "class")
```

Now we can build our confusion matrix and compute our first metrics, i.e. the number of correct predictions / total observations (the accuracy), or the prediction error.

```{r}
#| tidy: false

# confusion matrix
table(glm.pred, Direction) # predictions vs truth

# correct predictions / total observations
# "by hand" (the horror...)
(507 + 145) / 1250

# equivalent but more elegant
mean(glm.pred == Direction)

# and of course we could also compute the error ("!=" means "not equal to")
mean(glm.pred != Direction)
```

A "good" model should have high numbers along the main diagonal of this matrix: here, we see that this model could be improved, since a lot of "Down" are predicted as "Up" more often than it should be.

Now, the `tidymodels` version. First, meet the `augment()` function that adds the predictions (labels and probabilities) to the original dataframe:

```{r}
augment(lr_fit, new_data = dataf)
```

Note the new columns `.pred_class`, `.pred_Down`, and `.pred_Up`.

From this object we can compute the confusion matrix like this:

```{r}
augment(lr_fit, new_data = dataf) %>%
    conf_mat(truth = Direction, estimate = .pred_class)
```

Remember that so far we have been talking about the *training* performance! We trained and tested the model on the same set of observations.

In order to obtain a better and less unbiased assessment of our model, we need to know if it can be *generalized* to a new, unseen dataset. To do this, we are going to fit the model on a fraction of the observation and see how well it is predicting the *held out* fraction of observations.

Since we work with data having a time component, it is natural to fit the model using the first years and evaluate it on the last year.

```{r}

# Split dataset in training and test, i.e. training contains observations from 2001 to 2004
train <- (Year != max(Year)) # TRUE/FALSE result (Boolean)
dataf_train <- dataf[train, ]
dataf_test <- dataf[!train, ]
dim(dataf_test)

# We also save the 2005 directions only (for later comparison)
Direction_test <- Direction[!train]
```

We now fit a model using only the training data (`subset=train`), and then predict on the held out portion.

```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                data = dataf,
                family = binomial,
                subset = train)

glm.probs <- predict(glm.fits, dataf_test, type = "response")

glm.pred <- rep("Down", nrow(dataf_test))
glm.pred[glm.probs > 0.5] <- "Up"

# confusion matrix
table(glm.pred, Direction_test)
# accuracy
mean(glm.pred == Direction_test)
# error
mean(glm.pred != Direction_test)
```

Here is how to achieve the same result with the `tidymodels` approach. Note that we reuse the `lr_spec` model specification (since it is an abstract concept), changing only the fit and prediction parts:

```{r}
lr_fit_tr <- lr_spec %>%
    fit(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
        data = dataf_train
    )

augment(lr_fit_tr, new_data = dataf_test) %>% # this silently evaluates the model on dataf_test
    conf_mat(truth = Direction, estimate = .pred_class)

# to compute the accuracy:
augment(lr_fit_tr, new_data = dataf_test) %>%
    accuracy(truth = Direction, estimate = .pred_class)
```

We can make two important observations:

1.  The model is not performing so good, as the accuracy (48%) is worse than random guessing for binary classification (50%);
2.  This model performs worse than the first model: we expect this, since we trained it on fewer data.

What could we do to improve the model?

We can refit the model after removing predictors that seem to have small or no relationship with the target: in fact, such predictors will only cause an increase in variance without a decrease in bias. If you remember, the model's p-values were quite high, and the smallest one was associated to the predictor `Lag1`. Let's refit a model by considering only `Lag1` and `Lag2`:

```{r}
glm.fits <- glm(Direction ~ Lag1 + Lag2,
                data=dataf,
                family=binomial,
                subset=train)
```

```{r}
glm.probs <- predict(glm.fits, dataf_test, type="response")

glm.pred <- rep("Down", nrow(dataf_test))
glm.pred[glm.probs > 0.5] <- "Up"

table(glm.pred, Direction_test)
# accuracy
mean(glm.pred ==  Direction_test)
# accuracy of the Up class (precision)
106/(106+76)
prop.table(table(glm.pred, Direction_test)[2, ])
```

The model is starting to perform better.

Suppose now we want to predict the stock returns on a day when $\mathrm{Lag1}=1.2$, $\mathrm{Lag2}=1.1$ (day 1) and on a day when $\mathrm{Lag1}=1.5$, $\mathrm{Lag2}=-0.8$ (day 2). We first create a dataframe containing these values, and then use the `predict` function using the `newdata` argument:

```{r}
newdf <- data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8))
newdf # have a look at it

predict(glm.fits, newdata=newdf, type="response")
```

## K-Nearest Neighbors

We are using a different approach here: a K-Nearest Neighbor (KNN) model.

In R, the KNN is available through the `knn()` function in the `class` library. However, its syntax is different from `glm()` in that you do not fit and predict, but you get the predictions using a single command:

```{r}
#| eval: false

library(class)
knn.pred <- knn(x_train, x_test, y_train, k = 3)
```

Now we want to fit a KNN model with `K=3` neighbors on our stock market training data (`dataf_train`), using only `Lag1` and `Lag2` as predictors.

```{r}
library(class)
# prepare data
X_train <- dataf_train %>% select(Lag1, Lag2)
X_test <- dataf_test %>% select(Lag1, Lag2)
Direction_train <- dataf_train$Direction
```

```{r}
# fit KNN, get predictions
set.seed(1)
# knn.pred <- knn(---)
knn.pred <- knn(X_train, X_test, Direction_train, k=3)
```

```{r}
# confusion matrix
table(knn.pred, Direction_test)
# accuracy
mean(knn.pred == Direction_test)
```

> We set a random seed since there may be operations involving randomness (such as breaking a tie occurring in computing the nearest neighbors).
>
> Usually, whenever we need to perform operations involving randomness, in computer science we need to *set the seed* for the random number generator beforehand.
>
> This way we are sure to obtain reproducible results when running the code multiple times (with the same data, of course).

What about $k=5$?

```{r}
knn.pred <- knn(X_train, X_test, Direction_train, k=5)
```

```{r}
table(knn.pred, Direction_test)
mean(knn.pred == Direction_test)
```

Further increasing $k$ does not improve the predictions. On this data set, logistic regression performs better so far.

# New data set: Caravan Insurance Data

The `Caravan` data set (in the `ISLR2` library) includes 85 predictors measuring demographic characteristics of 5,822 individuals. The target variable is `Purchase`, indicating whether or not an individual is going to purchase a caravan insurance policy.

```{r}
dim(Caravan)
attach(Caravan)
summary(Caravan$Purchase)

# how many people purchased a policy?
prop.table(summary(Purchase))
```

In this dataset, only 6% of people purchased a caravan insurance.

Especially for KNN, the scale of a variable is very important, as KNN is based on *distances* to identify observations near to each other. So, variables on a large scale will impact the distance way more than variables on a smaller scale.

To quickly check the distributions of variables, you can run `summary(Caravan)`. Anyway, we need to standardize the data so that all variables have zero mean and unit variance - function `scale()`. When we do this, we should remember to exclude the categorical `Purchase` column!

```{r}
# standardizing - remove column Purchase
Caravan_scaled <- as.data.frame(scale(dplyr::select(Caravan, -Purchase)))
# check the variance of the first two predictors - before and after standardization
var(Caravan[, 1])
var(Caravan[, 2])

var(Caravan_scaled[, 1])
var(Caravan_scaled[, 2])

# now compare the means
mean(Caravan_scaled[, 1])
mean(Caravan_scaled[, 2])
```

It is time to split our observations into training and test sets. The test set will contain the first 1,000 samples. Afterwards we fit a KNN model ($k=1$).

Note that this is not the proper way to split samples: we'll see a better approach at the end of this lab.

```{r}
test <- 1:1000
# "test" is numeric, not boolean: therefore, use - and not ! for subsetting
Caravan_tr <- Caravan_scaled[-test, ]
Caravan_ts <- Caravan_scaled[test, ]

# split the target variable as well
Purchase_tr <- Purchase[-test]
Purchase_ts <- Purchase[test]
```

```{r}
set.seed(1)
knn.pred <- knn(Caravan_tr, Caravan_ts, Purchase_tr, k=1)

mean(Purchase_ts != knn.pred) # error rate
mean(Purchase_ts != "No") # average fraction of customers purchasing insurance
```

Let's have a closer look at the predictions, for example at the confusion matrix, and compute the accuracy for the Yes class:

```{r}
table(knn.pred, Purchase_ts)
# "the horror" version:
9 / (68 + 9)
# definitely better with...
prop.table(table(knn.pred, Purchase_ts)[2, ])
```

This is actually much better than random guessing. Let's try more KNN models, with $k=3$ and $k=5$:

```{r}
knn.pred <- knn(Caravan_tr, Caravan_ts, Purchase_tr, k=3)
table(knn.pred, Purchase_ts)

5/26
prop.table(table(knn.pred, Purchase_ts)[2, ])
```

```{r}
knn.pred <- knn(Caravan_tr, Caravan_ts, Purchase_tr, k=5)
table(knn.pred, Purchase_ts)

prop.table(table(knn.pred, Purchase_ts)[2, ])
```

The success rate is better and better as we increase $k$.

We can compare with a different model, such as Logistic regression. Compute as usual the confusion matrices and the accuracy for the Yes class, also trying different probability cutoffs.

```{r}
glm.fits <- glm(Purchase ~ .,
                data=Caravan_scaled,
                family=binomial,
                subset=-test)

glm_prob <- predict(glm.fits, newdata=Caravan_ts, type="response")

prob_cutoff <- 0.5 # usual cutoff
glm_pred <- rep("No", length(test))
glm_pred[glm_prob > prob_cutoff] <- "Yes"

table(glm_pred, Purchase_ts) # wow, no correct predictions for Purchase = Yes

prob_cutoff <- 0.25 # let's lower the cutoff
glm_pred <- rep("No", length(test))
glm_pred[glm_prob > prob_cutoff] <- "Yes"

table(glm_pred, Purchase_ts)
# of the 33 people predicted to buy a policy, we are correct for 11 of them
prop.table(table(glm_pred, Purchase_ts)[2, ])
```

# ROC curves

See how the performance changes with the probability cutoff? We can expect that the per-class error rate gradually improves, but at the same time the fraction of samples getting incorrectly classified also increases, as well as the overall error rate.

Depending on the specific case, one may be likely to accept a slight increase in the overall error rate, just to improve the classification accuracy of one class. The goal is to find that sweet spot between a low overall error rate and a low per-class error rate.

How can we achieve this goal? Suppose we test several thresholds for the posterior probabilities and save the performance metrics of the model for each threshold. Then, we can put them together in a plot that summarizes the two types of errors for all thresholds: that is, we can make a **ROC curve** (see page 150 in the textbook).

The ingredients of the ROC curve are:

-   *sensitivity* (or *recall*, or *true positive rate*) = TP / (TP + FN)
-   *specificity* (or *true negative rate*) = TN / (TN + FP)
-   *false positive rate* = FP / (FP + TN) = 1 - specificity

where TP (true positives), TN (true negatives), FP (false positives), and FN (false negatives) are the four cells of the confusion matrix: TP and TN stay on the main diagonal; FP and FN on the antidiagonal.

ROC curves are obtained by plotting the sensitivity vs. the false positive rate, which is 1 - specificity.

In our Caravan example, the "Positive" class corresponds to the target variable being "Yes" (people purchase a policy).

```{r}
# placeholders for storing sensitivity and specificity
all_sens <- all_fpr <- c()

# fit a logistic regression model on the training set
glm_fits <- glm(Purchase ~ .,
                data=Caravan_scaled,
                family=binomial,
                subset=-test)

# predict on the test set
glm_prob <- predict(glm_fits, newdata=Caravan_ts, type="response")
```

```{r}
# loop over 500 probability values,
# compute sensitivities & false positive rates
for (prob_cutoff in seq(0, 1, length.out=500)) {
    sens <- sum(glm_prob >= prob_cutoff & Purchase_ts == "Yes") / sum(Purchase_ts == "Yes")
    fpr <- sum(glm_prob >= prob_cutoff & Purchase_ts == "No") / sum(Purchase_ts == "No")

    all_sens <- append(all_sens, sens)
    all_fpr <- append(all_fpr, fpr)
}
```

```{r}
plot(all_fpr, all_sens, type = "l", xlab = "False positive rate", ylab = "True positive rate")
abline(0, 1, col = "gray", lty = 2)

# compute the AUC
height <- (all_sens[-1] + all_sens[-length(all_sens)]) / 2
width <- -diff(all_fpr) 
# alternatively:
# width <- diff(rev(all_fpr))
sum(height * width)
```

In the ROC curve plot, we also add the line corresponding to the "no information" classifier (e.g., random guess).

OK - we learned how to create a ROC curve "by hand", which is helpful to get a grasp on the underlying principle. Actually there are a number of R libraries that take care of automating this process, letting us focus on the results. One of these libraries (which we'll use in one of the next lab sessions) is `ROCR`. Let's see how all of the above code translates into the new library.

```{r}
#| tidy: false

if (!require(ROCR)) {
    install.packages(ROCR)
    library(ROCR)
}
```

```{r}
# first, we need to create a "prediction" object
# I specify ROCR:: to highlight that "prediction()" is part of this library
predob <- ROCR::prediction(glm_prob, Purchase_ts)
# then we need a "performance" object
perf <- ROCR::performance(predob, "tpr", "fpr")
# now the plot!
plot(perf)
abline(0, 1, col = "gray", lty = 2)
```

# Train/test splitting

In the exercise about `Caravan`, we split the full dataset into train/test partitions by simply taking apart the 1st 1,000 rows and putting them in the test set.

While this is a quick & dirty approach, it is not the best. In fact, doing so may introduce **bias** in the downstream analysis, since the partition is definitely not random.

A better way is to randomly select the indices for train/test sets: to do so, we'll use the function `sample()`. Remember to set the random seed to get reproducible results.

For example, let's randomly draw 10 items from an element of size 20:

```{r}
set.seed(1)
sample(20, 10)
```

Back to the Caravan data, here is a way to randomly split into train/test:

```{r}
# set the seed
set.seed(99)
# get 1000 random indices choosing from the number of samples in standardized.X
test <- sample(nrow(Caravan_scaled), 1000)
# then, select train and test as usual
Caravan_tr <- Caravan_scaled[-test, ]
Caravan_ts <- Caravan_scaled[test, ]

Purchase_tr <- Purchase[-test]
Purchase_ts <- Purchase[test]

knn.pred <- knn(Caravan_tr, Caravan_ts, Purchase_tr, k=1)
# confusion matrix
table(knn.pred, Purchase_ts)
# accuracy for the "Yes" class
prop.table(table(knn.pred, Purchase_ts)[2, ])

# repeat with increasing values of k
knn.pred <- knn(Caravan_tr, Caravan_ts, Purchase_tr, k=3)
prop.table(table(knn.pred, Purchase_ts)[2, ])

knn.pred <- knn(Caravan_tr, Caravan_ts, Purchase_tr, k=5)
prop.table(table(knn.pred, Purchase_ts)[2, ])
```

You can repeat the above using a different random seed: notice that you can get different results and that the accuracy of the positive class may not always increase with k.
