---
title: "Statistical Learning, Lab #2: LDA, QDA, Poisson regression"
author: "Marco Chierici"
date: "March 19, 2025"
format:
    html:
        theme: cosmo
        df-print: paged
        toc: true
        toc-location: left
knitr:
    opts_chunk:
        warning: false
        message: false
        tidy: true
        tidy.opts:
            width.cutoff: 60
editor_options:
  chunk_output_type: console
---

## Linear Discriminant Analysis

For this lab, we'll use the Stock market data again:

```{r}
library(tidyverse)
library(tidymodels)
library(ISLR2)

dataf <- Smarket
attach(dataf)
```

```{r}
# useful snippet to install a library on-the-fly if not already installed
if (!require(MASS)) { # require() is like library() but returns TRUE if the package exists or FALSE otherwise
    install.packages(MASS)
    library(MASS)
}

# alternative 1
# install.packages("pacman")
# library(pacman)
# p_load(MASS) # load the library if installed, or install & load it if not installed

# alternative 2
# install.packages("pak")
# pak::pkg_install("MASS")
# library(MASS)
```

As we previously did with Logistic Regression in Lab1, we fit the model using only observations before 2005 (identified by the `train` boolean variable) with only `Lag1` and `Lag2` as predictors, and we want to predict the market direction for 2005.

```{r}
# Split dataset in training and test, i.e. training contains observations from 2001 to 2004
train <- (Year < 2005) # TRUE/FALSE result (Boolean); parentheses are not necessary but improve readability
df.2005 <- dataf[!train, ]
dim(df.2005)

# We also save the 2005 directions only (for later comparison)
Direction.2005 <- Direction[!train]

lda.fit <- lda(Direction ~ Lag1 + Lag2,
               data = dataf,
               subset = train)

lda.fit

plot(lda.fit)
```

After we use `predict()` on new data, we get a list with three elements:

-   `class`, the predictions;
-   `posterior`, a matrix whose k-th column contains the posterior probability of an observation belonging to the class k;
-   `x`, the linear discriminants.

```{r}
# predict
lda.pred <- predict(lda.fit, newdata = df.2005)

# save the predictions in a new variable, just for convenience
names(lda.pred)
head(lda.pred$x)
lda.pred$class[1:10]
head(lda.pred$posterior)

lda.class <- lda.pred$class
# confusion matrix
table(lda.class, Direction.2005) #evaluation on test set

# accuracy
mean(lda.class == Direction.2005) #accuracy for the model
```

Since in the previous lab we fit a logistic regression model on the same data, we can repeat the analysis to quickly compare the performance:

```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2,
               data = dataf,
               subset = train,
               family = binomial)
```

```{r}
glm.probs <- predict(glm.fit, df.2005, type="response")
glm.class <- ifelse(glm.probs > 0.5, "Up", "Down")
# confusion matrix
table(glm.class, Direction.2005)
# accuracy
mean(glm.class == Direction.2005)
```

Let's go back to LDA. If we threshold the posterior probabilities using the usual 0.5 threshold, we obtain the predictions reported in `lda.pred$class`. Compare the number of elements whose probabilities are higher or lower than the threshold with the actual number of Up and Down predictions, in order to understand which market direction is predicted by LDA.

```{r}


```

```{r}
sum(lda.class=="Up")
sum(lda.class=="Down")
```

Alternatively, one may compare the probabilities vs. the actual labels for a few observations:

```{r}
lda.pred$posterior[1:20, 1]
lda.class[1:20]
```

We could also use a different threshold to make predictions: for example, 90% instead of 50%. This is easily done:

```{r}


```

## Quadratic Discriminant Analysis

Similarly, we are now fitting a QDA model to our `Smarket` data (stored in the `dataf` variable). QDA is implemented in the `qda()` function (`MASS` library), which has the same syntax as `lda()`:

```{r}
qda.fit <- qda(Direction ~ Lag1 + Lag2,
               data=dataf,
               subset=train)
qda.fit
```

As usual, we fitted a QDA model on the training portion of `dataf`.

Compare the output with LDA's. What's different?

We can `predict()` on unseen data (`df.2005`) exactly like we are used to:

```{r}
qda.pred <- predict(qda.fit, df.2005)

names(qda.pred)

qda.class <- qda.pred$class
# confusion matrix
table(qda.class, Direction.2005)
# accuracy
mean(qda.class == Direction.2005)
```

We reach an accuracy of about 60%: this is interesting, given that we didn't use the 2005 data to fit the model and that the stock market is known to be hard to model accurately.

## Poisson regression

This family of generalized linear models is particularly used for modeling events where the outcomes are counts, or in general *count data* (discrete data with non-negative integer values). A typical example is the number of times an event occurs in a specific timeframe. Note that count data can also be expressed as *rates* or *frequencies*.

So far, we have been experimenting on the `Smarket` data set: let's focus now on a new data set, `Bikeshare`.

```{r}
as_tibble(Bikeshare)
names(Bikeshare)
```

We will use these five variables as predictors to model the target variable `bikers`.

We start by fitting a *least squares regression model* to the data:

```{r}
mod_lm <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
             data=Bikeshare)
summary(mod_lm)
```

### About "contrasts"

A "contrast" is a linear combination of predictors that allows comparing different conditions. In R, the default contrast coding scheme is "dummy coding" (or "treatment coding"), where each level of a factor is compared to a fixed reference level (the 1st level of the factor). Here is what the dummy coding looks like on `Bikeshare`'s 12-level variable `mnth`:

```{r}
levels(Bikeshare$mnth)
contrasts(Bikeshare$mnth) #shows how the standard encoding is done automatically in R (the variable with all 0s is the baseline)
```

Another contrast coding scheme is "sum coding" (or "deviation coding"), where we compare the mean of the dependent variable for a given level to the overall mean of the dependent variable. For a 12-level variable, it looks like this:

```{r}
contr.sum(12) #number of levels as input and shows how R encodes this
#first 11 rows represent an identity matrix
#last row has -1 for every column
#column-wise the sum is always zero

```

### Revised contrasts for Bikeshare regression

In Section 4.6.1 of the textbook, the coefficients reported in Table 4.10 were obtained with a `lm` fit after coding the variables `hr` and `mnth` in a slightly different way:

```{r}
contrasts(Bikeshare$mnth) <- contr.sum(12)
contrasts(Bikeshare$hr) <- contr.sum(24)

```

This coding is made so that the coefficient estimates of the last levels of `hr` and `mnth` will be the negative of the sum of the coefficient estimates for all other levels. In other words, the coefficients of `hr` and `mnth` in the model fit will always sum to zero, and can be interpreted as the difference from the mean level.

```{r}
mod_lm2 <- lm(bikers ~ mnth + hr + workingday + temp + weathersit,
             data=Bikeshare)
summary(mod_lm2)
#on average there are 30 less bike rentals in march compared to the year average
#this is just a way to interpret the coefficients but it does not affect the model fit otherwise
```

Is this choice of coding affecting the model's predictions?

```{r}
sum((predict(mod_lm) - predict(mod_lm2))^2)
```

We could also check whether the two predictions are the same:

```{r}
all.equal(predict(mod_lm), predict(mod_lm2)) #the two models predict the same from the data
```

Now, let's reproduce Figure 4.13 of the textbook.

For the left-hand panel, we need the coefficient estimates of `mnth`:

```{r}
coef(mod_lm2)[2:12]

coef_months <- c(coef(mod_lm2)[2:12],
                 -sum(coef(mod_lm2)[2:12]))

# verify that all coefficients sum up to 0
sum(coef_months)
```

For the plot, we have to manually label the x-axis with the initials of the months:

```{r}
plot(coef_months, xlab = "Month", ylab = "Coefficient",
     xaxt = "n", # don't draw the x-axis as we are customizing it
     col = "blue", pch = 19, type = "o")

axis(side = 1,
     at = 1:12, # where to put ticks
     labels = c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D"))
```

A similar process can be applied to `hrs` for reproducing the right-hand panel of Figure 4.13:

```{r}
coef_hours <- c(coef(mod_lm2)[13:35],
                -sum(coef(mod_lm2)[13:35]))

plot(coef_hours, xlab = "Hour", ylab = "Coefficient", col = "blue", pch = 19, type = "o")
```

Now let's fit a Poisson regression model.

```{r}
mod_pois <- glm(bikers ~ mnth + hr + workingday + temp + weathersit,
                data = Bikeshare,
                family = poisson
                )
summary(mod_pois)
```

Regarding the coefficient estimates, we make the same considerations as for `mod_lm2`: the coefficients of the last levels of `mnth` and `hr` will be the negative of the sum of the coefficients of the other levels.

Now we can reproduce Figure 4.15:

```{r}
coef_mnth <- c(coef(mod_pois)[2:12],
               -sum(coef(mod_pois)[2:12]))

plot(coef_mnth, xlab = "Month", ylab = "Coefficient",
     xaxt = "n", # don't draw the x-axis as we are customizing it
     col = "blue", pch = 19, type = "o")

axis(side = 1,
     at = 1:12, # where to put ticks
     labels = c("J", "F", "M", "A", "M", "J", "J", "A", "S", "O", "N", "D"))

coef_hr <- c(coef(mod_pois)[13:35],
             -sum(coef(mod_pois)[13:35]))

plot(coef_hr, xlab = "Hour", ylab = "Coefficient", col = "blue", pch = 19, type = "o")
```

To obtain the fitted values from this Poisson regression model, we use `predict()` specifying `type="response"`. In this way, we will get our desired output

$$exp(\hat{\beta_0} + \hat{\beta_1}X_1 + \ldots \hat{\beta_p}X_p)$$

instead of

$$\hat{\beta_0} + \hat{\beta_1}X_1 + \ldots \hat{\beta_p}X_p$$

which we would get by default.

```{r}
pred_lm2 <- predict(mod_lm2)
pred_pois <- predict(mod_pois, type="response")

plot(pred_lm2, pred_pois, pch=19, cex=0.5, col=rgb(70, 130, 180, max=255, alpha=125))
abline(0, 1, col=2, lwd=3)
```
